0:00
aw certified AI practitioner this exam validates your
0:07
knowledge of artificial intelligence machine learning generative AI Concepts and use
0:14
cases an organization is developing a model to predict the price of a product based on various features like size
0:20
weight brand and manufacturing date which machine learning approach would be best suited for this task our options
0:28
are classification regression clustering and dimensionality
0:35
reduction option b regression is the right answer here clustering groups similar data
0:43
points together dimensionality reduction reduces the number of features regression predicts a continuous
0:49
numerical value and classification predicts categorical
0:55
outcomes a company is expanding its use of artificial intelligence which which core principle should they prioritize to
1:02
establish clear guidelines oversight and accountability for AI development and use our options are bias prevention
1:10
accuracy and reliability data protection and security or
1:17
governance option D governance is the right answer here it encompasses the
1:24
overall framework for managing AI systems including policies procedures
1:29
and decision making process other options are important too but for oversight governance is the best
1:36
answer here a company is starting to use generative AI on AWS to ensure
1:42
responsible AI practices which tool can provide them with guidance and information option A AWS Marketplace B
1:51
AWS AI service cards C Sage maker and D bedrock
2:00
option b AWS AI service cards is the right answer here AI service cards offer
2:06
detailed information on specific awsi services including their intended use cases limitations and responsible design
2:14
considerations AWS Marketplace is a platform for buying and selling AI services and tools AWS sagemaker is a
2:22
machine learning implementation service AWS Bedrock provides access to a
2:29
variety of Foundation models what is the primary purpose of feature Engineering in machine
2:36
learning option A to ensure consistent performance of the model B to evaluate
2:41
the model's performance C to gather and pre-process data features D to transform
2:48
data and create variables or features for the
2:54
model option D to transform data and create variables or features for the model is the right answer here a small
3:02
company wants to use machine learning to predict customer churn but they lack an expert dedicated data science team which
3:09
AWS tool can help them build models easily without extensive coding option A
3:15
Amazon sagemaker jumpstart B sagemaker Studio C sagemaker canvas or D sagemaker
3:23
data rangler option C sagemaker canvas is the
3:31
right answer here it is a visual interface for building machine learning models without
3:36
writing code sagemaker jumpstart provides pre-built models and templates
3:42
sagemaker studio is a web-based IDE for machine learning development sagemaker
3:47
canvas is a visual interface for building machine learning models without writing code and sagemaker data randler
3:54
prepares and cleans data for machine learning a financial institution is
4:00
developing a fraud detection model the Project Lead announced that they would be using mlops how would you explain
4:07
mlops in the context of this project option A a tool for visualizing ml model
4:13
performance b a set of practices for managing the entire life cycle of ml
4:18
systems c a process for deploying and maintaining ml models in production d a
4:24
framework for building and training ml models
4:31
option b a set of practices for managing the entire life cycle of ml systems is the right answer here ml Ops encompasses
4:39
the full life cycle including development deployment monitoring and
4:44
maintenance of machine learning models which aw service can be used to
4:50
create a knowledge-based chatboard that can answer questions about company's products and services using the
4:56
company's internal documents as a source of information option A Amazon sagemaker B Amazon Q
5:04
business C Amazon poly or D Amazon
5:12
recognition option b Amazon Q business is the right answer here it allows
5:18
businesses to create intelligent chat Bots and search interfaces that can answer questions based on their
5:24
Enterprise documents and data sagemaker is a managed platform for
5:29
for building training and deploying machine learning models Amazon poly is a text to speech
5:35
service that converts text into natural sounding speech recognition is a deep
5:42
learning based image and video analysis service a development team needs to
5:48
select a service for storing and quaring Vector embeddings which aw service is
5:53
best suited for this option a glue data catalog B Amazon
5:59
S3 C red shift or D open search
6:06
service option b Amazon open search service is the right answer
6:11
here here are some services that support vector-based storage and search Amazon
6:17
open search Amazon memory DV Amazon Aurora RDS postgress Amazon Kendra and
6:25
there are other products like pine cone Milas vv8 and
6:30
qra an organization wants to evaluate the security and compliance practices of AWS Services used by vendors selling AI
6:38
products which AWS service can help them access AWS compliance reports and
6:43
certifications our options are AWS organization Amazon inspector AWS cloud
6:50
trail or AWS artifact option D ews artifact is the
6:56
right answer here it allows users to access a centralized repository of AWS
7:02
compliance reports attestations and certifications this includes reports related to security privacy and
7:09
Regulatory Compliance the other options AWS organization Amazon inspector or AWS
7:15
cloud trail are not relevant here a machine learning model performs well on
7:22
training data but poorly on new data what is the likely problem option A
7:28
overfitting B underfitting C insufficient training data or D poor
7:34
data quality option A overfitting is the right answer here so overfitting is the
7:42
most likely cause of high accuracy on training data but low accuracy on
7:48
testing data this occurs when a model becomes overly complex and learns the training data too well to the point
7:55
where it can't generalize to new or unseen data all the other options b c
8:02
and d will result in poor accuracy on both the training and
8:08
newator a company wants to improve the quality of large language model responses by accessing external
8:15
information which method requires the least amount of development effort option a few shot learning B zero shot
8:23
learning C retrieval augmented generation or rag or D fine-tuning
8:32
option C retrieval augmented generation is the right answer here it can retrieve
8:38
and incorporate external information into prompts that are sent to
8:45
llm a model has been trained to recognize handwritten digits in images
8:50
however the model is not accurate a ml expert has advised that Epoch value should be increased what is
8:58
Epoch in the context of machine learning option A A measure of accuracy of a
9:04
model during training option b a single pass through the entire training data set by the model option C the process of
9:12
splitting the data set into training and testing sets option D number of lays in
9:18
a neural network option b is the right answer
9:24
here a single pass through the entire training data set by the model during each
9:30
the model processes all the data once adjusting its parameters based on the
9:35
loss or error calculated after each pass multiple epochs are typically used to
9:41
iteratively improve the model's performance which of the following is
9:46
considered a hyperparameter in a machine learning model option A weights of the
9:52
neural network B learning rate of the optimization algorithm C output of the
9:58
activation function or D predictions made by the
10:04
model option b learning rate of the optimization algorithm is the right
10:09
answer here in machine learning the learning rate is a hyperparameter that determines the size of the steps the
10:16
model takes when adjusting its weights during training specifically it controls how much to change the model's
10:22
parameters in response to the error or loss between the predicted output and
10:28
the actual Target so choosing an appropriate learning rate is crucial for the effective training
10:35
and convergence of the model other options a c and d do not refer to hyper
10:44
parameters a model tends to give very similar outputs even when you vary the input slightly which inference time
10:51
parameter can be adjusted to make it a little more creative option a learning
10:56
rate B batch size C temperature or D
11:04
epox option C temperature is the right answer here it controls the randomness of predictions by adjusting the
11:10
probability distribution used during sampling lower temperature makes the
11:15
model more deterministic while higher temperature increases Randomness and
11:22
diversity other options a b and d that is learning rate batch size and EPO
11:30
these are parameters that are used at training time and not inference
11:35
time so here are training time hyperparameters learning rate controls
11:40
the step size taken during optimization batch size determines the number of
11:46
samples processed at once during training epox refers to the number of
11:51
times the entire data set is passed through the model during training it can improve model's accuracy regularization
11:59
these are techniques used to prevent overfitting and Optimizer is algorithm
12:04
used to update model weights here are inference time parameters temperature controls the
12:11
randomness of generated text top K sampling selects only the top K most
12:18
likely tokens for output top P sampling selects tokens for output based on their
12:25
cumulative probability beam surch explores multiple candidate sequences
12:31
greedy surge selects the most likely token for output at each
12:36
step you are evaluating a language generation model on various tasks related to text generation to assess the
12:43
quality of the generated text which evaluation metric best measures its semantic similarity to human written
12:50
text our options are a bird score B Blue
12:55
C perplexity or D Rouge
13:03
option A Bird score is the right answer here this metric leverages a pre-trained
13:08
language model to calculate semantic similarity between text it's particularly well suited for tasks like
13:15
machine translation and summarization where semantic meaning is crucial a developer is designing an AI
13:23
system and needs a solution that provides comprehensive tools for analyzing and explaining model pred
13:29
predictions which aw service is specifically designed to enhance transparency and explainability in this
13:36
context option A sagemaker clarify B sagemaker debugger C sagemaker autopilot
13:44
or D sagemaker data Wrangler option A sagemaker clarify is
13:51
the right answer here sagemaker clarify provides tools for analyzing model
13:56
predictions and detecting bias enhan ing transparency and explainability sagemaker debugger provides tools for
14:03
debugging and profiling machine learning models sagemaker autopilot automatically trains and tunes machine learning models
14:11
sagemaker data Wrangler simplifies data preparation and transformation for machine
14:17
learning a company plans to train and build Its Own Foundation model what are
14:22
potential drawbacks of this approach against using a pre-trained foundation model option a more complex
14:30
implementation process b reduced performance C risk of higher
14:36
hallucination D increased development
14:43
cost option A and D are the right answers here that is more complex
14:48
implementation process and increased development cost a company wants to generate content
14:55
using an existing popular pre-trained AI model they have limited AI expertise and
15:00
don't want to manage the model themselves which aw service would best suit their needs option A Amazon
15:07
textract B Amazon comprehend C Amazon Bedrock or D Amazon Sage
15:16
maker option C Amazon Bedrock is the right answer here Bedrock is a managed
15:22
service that provides access to pre-trained Foundation models making it ideal for companies that want to
15:27
leverage a without the need for extensive technical expertise or
15:34
infrastructure Amazon textract extracts text handwriting and data from scan
15:40
documents Amazon comprehend analyzes text for sentiment entities key phrases
15:46
and more Amazon Bedrock provides access to pre-trained Foundation models for
15:52
tasks like text generation Amazon sagemaker is a fully managed platform
15:57
for building training and deploying machine learning models what type of training data would
16:04
be most suitable to fine tune a model to respond to questions in a certain format and style option A columnar data set B
16:12
label data C transcription logs D text PS of prompts and
16:19
responses option D text PS of prompts and responses is the right answer here
16:25
this will allow the model to learn to respond to prompts in a PR preferred form and
16:30
style a company needs to log API calls to Amazon Bedrock for compliance
16:36
including details about the API call the user and the timestamp which aw service
16:41
can assist with this option a cloud trail B Cloud watch C IM am and d
16:49
security Hub option a cloud trail is the right
16:55
answer here it can record API calls made to AWS services including the details of
17:01
the API call the user who made it and the timestamp making it suitable for
17:07
tracking API activity for compliance purposes Amazon cloudwatch monitors and
17:13
collects metrics and logs I am and security Hub are security related
17:20
Services a data science team wants to improve a model's performance they want to increase the amount and diversity of
17:27
data used for training and ify the algorithm's learning rate which combination of ml pipeline steps will
17:33
meet these requirements option a data augmentation B model monitoring C feature engineering
17:41
or D hyperparameter
17:48
tuning option A and D data augmentation and hyperparameter tuning is the right
17:53
answer here data augmentation is a technique used to artificially increase the the size and diversity of a data set
18:01
by applying various transformations to existing data points thereby providing the model with more examples to learn
18:08
from hyperparameter tuning involves adjusting the learning rate which is a crucial parameter that controls how
18:15
quickly the model updates its weights during training model monitoring does not help
18:22
here feature engineering involves selecting or creating new features from data for training and is not relevant
18:30
here a company wants to ensure that the content generated by their Amazon Bedrock powered application aderes to
18:36
their ethical guidelines and avoids harmful or offensive content which aw
18:42
service can help them Implement these safeguards option A Amazon sagemaker B
18:48
Amazon comprehend C textract or D guard rails for Amazon
18:56
bedro option D guard rails for Amazon Bedrock is the right answer here guard
19:02
rails ensure responsible use of foundation models aligning with organizational compliance requirements
19:08
it prevents harmful biased or inappropriate content Generation by filtering and monitoring
19:15
outputs sagemaker is a fully managed platform for building training and deploying machine learning models
19:21
comprehend analyzes text for sentiment entities key phrases and more textract
19:27
extracts text handwriting and data from scan documents your company is training a
19:34
machine learning model on a data set stored in s S3 that contains sensitive customer information how can you ensure
19:40
that any sensitive information in the data is removed or anonymized before trading the model option A use S3
19:48
encryption to protect the data at rest option b use Amazon Mai to identify
19:54
sensitive information within the data set option C use S3 access controls to
20:01
limit access to authorized Personnel option D Implement data masking techniques to replace sensitive
20:13
information option b and d are the right answers here that is use Amazon Macy to
20:20
identify sensitive information within the data set and Implement data masking techniques to replace sensitive
20:27
information other options help secure the data but are not relevant to removing or anonymizing sensitive data a
20:36
company wants to use generative AI to create marketing slogans for their products why should the company
20:41
carefully review all generated slogans option A generative AI May generate
20:46
slogans that are too long and difficult to remember option b generative AI May
20:52
struggle to capture the unique brand identity of the company option C
20:57
generative AI produce slogans that are inappropriate or misleading option D generative AI may require extensive
21:04
training data to generate effective
21:10
slogans option C generative AI May produce slogans that are inappropriate or misleading is the right answer here
21:18
gen AI can sometimes generate content that is biased or offensive this may be due to the biases present in the data
21:25
they are trained on therefore human oversight is essential to ensure the quality and appropriateness of the
21:33
generated slogans your company is training machine learning models on ec2 instances you are
21:39
concerned about the security of these models and want to identify potential vulnerabilities in the underlying
21:45
infrastructure which AWS service can help you scan your ec2 instances for
21:50
vulnerabilities options AWS x-ray Amazon cloudwatch Amazon
21:56
inspector or AWS confence
22:03
option C Amazon inspector is the right answer here it is a security service
22:08
that helps you identify vulnerabilities in your ec2 instances AWS xray allows you to tras
22:17
requests as they flow through your application components Amazon cloudwatch is a monitoring service that provides
22:23
metrics logs and alarms for your AWS resources AWS config is a service that
22:30
tracks configuration changes to your AWS resources a machine learning model for
22:36
loan approvals performs better for applicants from urban areas because the training data contains more approval
22:43
examples from urban areas what type of bias is this an example of option a
22:50
sampling bias B algorithm bias C Observer Bias or D recency bias
22:59
option a sampling bias is the right answer here sampling bias occurs when the training data is not representative
23:07
of all scenarios algorithm bias happens when the algorithm itself
23:12
is biased observe bias the person collecting or labeling data introduces
23:18
bias recenty bias the model gives more weight to recent
23:24
data for a data set of social network connections where each each user has relationships with multiple other users
23:32
which machine learning algorithm is most suitable for classifying these interconnected relationships into
23:38
predefined categories option a linear regression B
23:43
decision trees C graph neural networks or D logistic
23:51
regression option C graph neural networks is the right answer here linear regression
23:59
predicts a continuous Target variable based on linear relationships between features decision trees classifies data
24:06
by recursively splitting it into subsets based on feature values forming a tree like model of
24:12
decisions graph neural networks classifies interconnected data by leveraging graph structures to capture
24:19
relationships between nodes logistic regression predicts the probability of
24:25
binary outcome by modeling the relationship between features and the outcome using a logistic
24:31
function a robot is tasked with navigating a maze to reach a goal which machine learning Paradigm would be most
24:37
suitable for training the robot to learn the optimal path via self-learning trial
24:43
and error our options are supervised learning unsupervised learning random
24:49
learning or reinforcement
24:55
learning option D reinforcement learning is the right answer here supervised learning trains a model
25:02
to predict outputs based on labeled input data unsupervised learning trains
25:08
a model to find patterns or relationships within unlabeled data reinforcement learning trains a model to
25:15
make decisions based on rewards and punishments received from its
25:21
environment a researcher wants to adapt a pre-trained machine learning model to perform well on a new domain specific
25:27
task with limited label data which of the following approaches would be most
25:33
efficient and suitable option A continued pre-training with additional
25:38
unlabeled data option b fine-tuning with labeled data from the new domain option
25:44
C using a pre-trained model without any further adjustment option D training
25:51
from scratch with new label data option b is the right answer here
26:00
fine tuning is appropriate for adopting a model to perform well on a specific task with label data it caters to domain
26:08
specific training most efficiently it could also be referred as domain adaption fine
26:16
tuning if you are a small startup with unpredictable workloads and need to experiment with the different Foundation
26:23
models which pricing model would be most suitable for you on Amazon bedrock
26:29
option A on demand B provision throughput C model customization D
26:36
custom contracts option A On Demand is the
26:44
right answer here on demand Pay As You Go for flexible usage ideal for
26:49
unpredictable workloads provision throughput reserved capacity for predictable workloads ensuring
26:55
consistent performance model custom ization training models on your data for specific tasks enhancing performance and
27:04
knowledge in the context of natural language processing which of the following is a fundamental unit of text
27:10
used to represent words or subwords option a token B Vector
27:16
embedding c engram d
27:22
vocabulary option a token is the right answer here token is the basic unit of
27:29
text used to represent words or subwords after tokenization Vector embedding is a
27:35
numerical representation of tokens or words in a continuous Vector space engram is a sequence of n consecutive
27:43
words or characters from a text used to analyze and understand patterns in
27:48
language vocabulary is a collection of all unique words that a model recognizes
27:54
and processes a developer is creating an system to predict customer churn to
28:00
ensure transparency they need to document key details about the model which AWS tool is best suited for this
28:06
task option A Amazon sagemaker clarify B AWS AI service cards C sagemaker model
28:14
cards and D sagemaker jump
28:19
start correct answer here is option C Amazon sagemaker model
28:25
cards AWS AI service cards provides information about different AWS AI
28:31
Services Amazon sagemaker clarify helps identify and mitigate biases in machine
28:37
learning models Amazon sagemaker jump start provides pre-built models and
28:42
templates for common machine learning tasks and Amazon seed maker model
28:48
cards documents and shares information about machine learning models such as
28:53
their intended use training data and evaluation metrics
28:58
an engineer is training a machine learning model in order to prevent underfitting or overfitting how should
29:04
the model be trained with data option A with high bias and high variance B low
29:10
bias and low variance C High bias and low variance d low bias and high
29:19
variance option b with low bias and low variance is the right answer here the
29:24
goal is to strike a balance between bias and variance when training model low bias ensures that the model
29:31
can accurately learn the underlying patterns in the data preventing underfitting low variance ensures that
29:37
the model generalizes well to new unseen data preventing
29:42
overfitting model overfitting is a case of low bias and high variance here the
29:48
model learned training data too well therefore it is unable to generalize on test data and the model has become
29:55
complex model underfitting is a case of high bias and low variance here the
30:01
model did not learn enough from training data its performance is spor both on
30:07
training and test data it is a simplistic model bias refers to models failure to
30:14
capture patterns in training data and variance refers to model sensitivity to
30:20
fluctuations and noise in data you are customizing a large
30:25
language model for a specific domain which approach is most effective for tailoring the models knowledge and
30:31
accuracy to this domain option A fine-tuning B few short learning C
30:38
retrieval augmented generation or rag D zero short
30:46
learning option a fine tuning is the right answer here here we train the
30:52
pre-trained model on domain specific data to improve its performance and knowledge with respect to that
31:00
domain fine-tuning trains the pre-trained model on domain specific data to improve its performance and
31:06
knowledge retrieval augmented generation or rag enhances the capabilities of a
31:11
large language model by incorporating information from external sources few
31:17
shot learning trains the model on a small amount of data zero shot learning
31:22
makes predictions on tasks or domains that the model hasn't been explicitly trained on
31:29
which of the following is an example of hallucination in large language models a overfitting b underfitting c
31:38
generating false or misleading information or D
31:44
bias option C generating false or misleading information is the right answer here large language models can
31:52
sometimes generate responses that are factually incorrect inconsistent or simply nonsensical
31:58
this is known as hallucination which is a foundation model developed by Amazon available wi
32:06
Bedrock options Amazon Titan Lex poly or
32:15
connect option A Amazon Titan is the right answer here Amazon Titan is a powerful
32:22
Foundation model from Amazon Amazon Lex this is a service for building conversational interfaces such
32:29
as chatbots and voice assistants Amazon poly is a service for
32:35
converting text to speech and Amazon connect is a cloud-based contact center
32:42
service which of the following algorithms are commonly used for classification tasks in machine
32:48
learning our options are support Vector machine XG boost K means or mean shift
32:58
option A svm and B XG boost are the right answers
33:04
here both svm and XG boost are supervised learning algorithms primarily
33:10
used for classification while K means and mean shift are unsupervised learning
33:16
algorithms primarily used for clustering given a large data set
33:22
intended for inference where latency is not a factor which sagemaker model inference type would you choose for cost
33:29
effective predictions our options are real time badge on demand serverless or
33:37
asynchronous option b batch is the right answer here it is good for processing
33:43
large data sets offline in a cost-effective manner real time is good for immediate predictions with low
33:49
latency On Demand serverless is ideal for workloads which have idle periods between traffic Spurs and can tolerate
33:56
cold starts no need to manage infrastructure the asynchronous method cues incoming requests and processes
34:03
them asynchronously it is good where processing times are long and queuing
34:08
requests make sense what is the primary purpose of Amazon Q developer our options are
34:16
option A to manage aw's infrastructure B to assess developers with coding tasks
34:21
and queries C to optimize database performance D to automate software
34:26
testing option b to assist developers with
34:32
coding tasks and queries is the right answer here Amazon Q developer is an AI
34:37
powered coding companion designed to assess developers with various coding tasks and queries it can help with code
34:44
generation debugging and answering questions about AWS services and best practices what kind of prompt attack is
34:52
this explain why a false statement is true considering that it's usually known
34:58
to be false option a jailbreaking B prompt
35:04
poisoning C adversarial prompting or D
35:10
fine-tuning option C adversarial prompting is the right answer here jailbreaking is about crafting inputs to
35:18
bypass a model safety constraints leading to prohibited or harmful responses prompt poisoning is about
35:25
introducing biased or misleading data into prompts to influence or degrade the model's
35:31
output adversarial prompting is about creating inputs designed to exploit model weakness and produce incorrect or
35:40
unintended responses you are building a text summarization tool which metric is best
35:47
for measuring how well it captures the key points of the original text our options are a bird score B Rouge c word
35:57
er eror rate D bilingual evaluation understudy or
36:06
blue option b Rouge is the right answer here it is specifically designed for
36:12
evaluating text summaries blue metric is used to evaluate the quality of text which has
36:18
been machine translated from one natural language to another bird score measures
36:23
semantic similarity between text Rouge is specifically Des designed for evaluating text summaries word error
36:30
rate is primarily used in speech recognition to measure the accuracy of
36:36
transcriptions complexity measures the complexity of a language model an eii
36:42
customer service agent is unable to accurately identify customer intent based on customer message you can
36:48
improve its performance by using training data in which format option a
36:54
customer message and customer intent B customer message and agent response C customer intent and agent
37:02
response D agent response and customer
37:10
intent option a customer message and customer intent is the right answer here
37:16
because this format directly provides the model with the information it needs to learn the relationship between
37:23
customer messages and their corresponding intents
37:28
how are users typically charged for using a foundation model option A number
37:33
of input tokens B number of output tokens C model architecture or D
37:40
inference latency option A and B is the right
37:47
answer here users are generally charged based on the number of tokens processed
37:53
during input and output in the models inference process model architecture and
37:59
inference latency are not typically used to charge model users which AWS AI service can be used
38:06
to extract Health Data from unstructured text such as clinical notes and medical
38:12
records option A Amazon comprehend medical B Amazon transcribed medical C
38:19
Amazon Health lake or D Amazon
38:26
recognition a Amazon comprehend medical is the right answer here Amazon comprehend medical extracts
38:34
medical information from text Amazon Health Link stores and analyzes Health
38:40
Data Amazon transcribe medical converts medical speech to
38:46
text which type of machine learning model is specifically designed to generate new data that resembles
38:52
existing data option A Auto encoder B generative adversarial net Network C
38:58
decision tree or D support Vector machine option b generative adversarial
39:05
network is the right answer here it is specifically designed for generative tasks such as creating new images music
39:12
or text autoencoders are used for dimensionality reduction and data
39:17
reconstruction decision trees and svms are primarily used for classification
39:22
and regression tasks users are going to use long prompts to ask questions from
39:28
their large language model what key aspect should be considered while selecting the llm to use option a
39:35
inference latency B maximum context window C model size or D training
39:44
data option b maximum context window is the right answer here a large maximum
39:51
context window allows the llm to process and understand more information from the
39:57
prompt leading to more comprehensive and relevant response which of the following best
40:04
describes the primary purpose of Amazon sagemaker feature store option A to
40:09
automatically train and deploy machine learning models B to store and manage
40:14
features for machine learning workflows C to provide a Marketplace for
40:19
pre-trained machine learning models D to optimize the performance of sagemaker training
40:25
jobs option B to store and manage features for machine learning workflows is the right answer here Amazon
40:32
sagemaker feature store is a fully managed purpose-built repository to store update retrieve and share Machine
40:39
learning features this ensures enabling feature sharing among data scientists
40:46
and machine learning engineers and ensuring feature consistency across training and
40:53
inference a healthcare organization is developing an AI power diagnostic tool to assist in early detection of a rare
41:00
disease with respect to Regulatory Compliance concerns which of the following is least relevant option A
41:08
ensuring the AI system is unbiased and does not discriminate against certain patient
41:14
demographics B minimizing operational expenses of the AI system C ensuring the
41:19
AI system is transparent in its decision-making process D preventing the
41:24
AI system from being used for unauthorized purposes
41:30
option b minimizing operational expenses of the AI system is the right answer here Regulatory Compliance in healthcare
41:38
especially for diagnostic tools focuses on factors like unbiased decision- making transparency and security while
41:45
minimizing operational expenses is important for overall efficiency it is less relevant to Regulatory Compliance
41:53
compared to other options you are a large Enterprise with massive amount of unstructured data
41:59
scattered across various internal systems you want to provide your employees with a powerful Search tool
42:05
that can understand natural language queries and return accurate relevant results which aw service would best meet
42:13
this need option A Amazon red shift B Amazon Lex C Amazon Kendra or D Amazon Dynamo
42:23
DB option C Amazon Kendra is the right answer here it is a managed service for
42:29
searching unstructured data using natural language queries Amazon red shift is a data
42:34
warehouse service Amazon Lex is a service for building conversational interfaces with natural language
42:40
understanding Amazon Dynamo DB is a nosql database service for storing and retrieving data a data scientist is
42:48
working on a project that requires rapid prototyping and experimentation with various machine learning algorithms
42:54
which AWS service would be most suitable for the askask option A Amazon sagemaker
42:59
ground truth B Amazon elastic compute cloud or ec2 C Amazon sagemaker
43:06
autopilot or D Amazon Bedrock option C Amazon sagemaker
43:14
autopilot is the right answer here it is a fully managed machine learning service that automates the process of building
43:20
training and deploying models Amazon sagemaker ground truth is a service for creating high quality
43:27
training data sets for machine learning models ec2 provides scalable compute
43:33
capacity in the cloud and Amazon Bedrock is a fully managed service that makes
43:38
foundational models accessible to developers a large company wants to
43:44
create an application for their sales managers that can reason perform multi-step tasks and provide insightful
43:50
responses from their Enterprise data which aw service would be most suitable for this task our options are Amazon Lex
43:58
sagemaker Bedrock knowledge bases or Bedrock
44:04
agents option D Amazon Bedrock agents is the right answer here they can leverage
44:10
both llms and knowledge bases to provide answers based on Enterprise data Amazon
44:17
Lex is specifically designed for building conversational interfaces sagemaker is a platform for
44:23
building and training machine learning models while knowledge basis can serve as a data source for other
44:31
services a company wants to analyze customer reviews to identify common themes and sentiments which aw service
44:39
can the company use to meet this requirement our options are Amazon connect comprehend translate or
44:51
transcribe option b Amazon comprehend is the right answer here
44:57
it can extract insights from text including entities sentiments and key
45:03
phrases Amazon connect is used for building contact centers Amazon
45:08
translate translates text between languages while Amazon transcribe can
45:14
transcribe audio into text a company wants to transform data
45:20
from one format to another to prepare it for machine learning tasks which aw service is best suited for this data
45:28
transformation AWS blue Amazon translate AWS config or Amazon
45:36
kesis option A AWS glue is the right answer here AWS glue is a fully managed
45:43
ETL service that can be used to transform data from one format to another Amazon translate is used for
45:50
translating text between languages AWS config is used for tracking configuration changes in AWS resources
45:57
and Amazon Kinesis is a service for processing realtime data streams a company wants to deploy a
46:04
trained machine learning model for real-time inference which aw service would be most suitable for this purpose
46:11
option A Amazon sagemaker jump start B personalize C ec2 or D sagemaker end
46:22
points option D sagemaker endpoints is the right answer here
46:27
it is specifically designed for real-time inference allowing you to deploy your trained machine learning model and make predictions in real time
46:35
sitemaker jumpstart provides pre-trained models and algorithms Amazon personalized is a
46:42
service for building personalized recommendations Amazon ec2 is a general
46:47
purpose compute service a company has deployed a machine learning model for customer sentiment
46:53
analysis to ensure the model's accuracy and reliability which AWS Services should be used for monitoring and human
46:59
review option A Amazon Bedrock B sagemaker model monitor C sagemaker
47:06
ground Tru or D Amazon
47:12
a2i so option b Amazon sagemaker model Monitor and option D Amazon a2i are the
47:19
right answers here sagemaker model monitor is a service for monitoring the performance of machine learning models
47:25
in production Amazon a2i or Amazon augmented AI is a service for
47:31
incorporating human review into machine learning workflows Amazon Bedrock
47:36
provides access to a variety of foundation models while Amazon ground truth is a service for creating
47:43
highquality training data sets for machine learning models a machine learning specialist is
47:49
training a large deep learning model on a massive data set in Amazon sagemaker a
47:54
single GPU may not handle this well which sagemaker feature can help optimize the training process for large
48:01
models and data sets option A incremental training B hyperparameter
48:07
tuning c pipe mode or D model parallelism option D model parallelism
48:15
is the right answer here it splits a large model across multiple gpus or
48:20
instances allowing each GPU to process a portion of the model incremental
48:26
training is useful for updating a model with new data hyperparameter tuning
48:31
optimizes models performance by trying different combinations of
48:37
hyperparameters and pipe mode is a feature for streaming data and efficient data management during
48:43
training you are working with a large data set with many features to improve your models performance and
48:49
computational efficiency you need to simplify the data without losing significant information which technique
48:55
would be most effective for achieving this goal option a dimensionality
49:00
reduction B feature engineering C data augmentation D data
49:10
cleaning option a dimensionality reduction is the right answer here this
49:16
technique reduces the number of features in a data set while preserving essential information feature engineering involves
49:23
creating new features from existing ones to improve model performance data
49:28
augmentation is a technique that creates additional training data by modifying existing data such as rotating images or
49:36
adding noise to audio data cleaning involves removing or correcting errors
49:42
inconsistencies or missing values in the data you want to generate highly
49:47
detailed images based on text descriptions which AI model specifically designed for generative tasks and
49:54
capable of producing highquality diverse outputs would be most suitable for this task option A generative adversarial
50:02
networks B recurrent neural networks C convolutional neural networks or D
50:09
stable diffusion option b stable diffusion is
50:16
the right answer here generative adversarial networks is
50:22
a class of machine learning models that use a competitive process to generate new data recurrent neural networks is a
50:29
type of neural network that can process sequential data such as text or time series convolutional neural networks are
50:37
a type of neural networks that are particularly effective for processing images and other grid like
50:44
data while stable diffusion is a model that is specifically designed for
50:50
generative tasks such as image generation a company has a system that
50:55
generates Vector embedding from product data they want to improve the speed and accuracy of finding similar products
51:01
which AWS services are best suited for implementing Vector search to optimize the system option A open search service
51:09
B red shift C Neptune or D document
51:16
DB option A C and D are the right answers here open search Neptune and document DB
51:24
support handling Vector data and performance pering similarity searches via Vector search Red shift is a data
51:31
warehousing service a bank receives numerous loan
51:37
applications daily the loan processing team manually extracts information from these applications which is timec
51:42
consuming the goal is to automate this process using AI tools which aw service
51:48
would be useful here option A Amazon recognition b textract c translate or D
51:55
transcribe option b Amazon textract is the right
52:01
answer here Amazon textract can extract text from scan
52:07
documents Amazon recognition analyzes images and videos to identify objects
52:13
scenes and faces Amazon translate translates text between languages and
52:18
Amazon transcribe can transcribe speech to text a Healthcare company wants to
52:25
develop a machine machine learning model to predict the likelihood of a patient developing diabetes based on various
52:31
health indicators which of the following metrics would be most appropriate for evaluating the model's performance our
52:38
options are accuracy precision F1 score recall sensitivity or area under Roc
52:47
curve options D and E are the right answers here here we are dealing with
52:53
the classification problem in this scenario both recall sensitivity and Au
53:00
would be appropriate metrics for evaluating the model's performance accuracy measures the
53:07
overall correct predictions Precision measures the proportion of positive predictions that are actually positive
53:14
F1 score is a harmonic mean of precision and recall providing a balanced measure
53:19
of performance recall sensitivity measures the proportion of actual positive instances that were correctly
53:27
predicted area under Roc curve or Au measures the model's ability to
53:33
distinguish between positive and negative instances an organization has trained a
53:39
deep learning model on a large data set of General images they now want to apply the same model to classify medical
53:45
images with a smaller data set which machine learning technique would be most suitable in this scenario our options
53:52
are reinforcement learning transfer learning supervised learning or unsupervised
54:01
learning option b transfer learning is the right answer give it is a technique
54:06
that reuses a pre-trained model for a new but related task often with some
54:12
fine-tuning on new data reinforcement learning is a type of machine learning
54:17
where an agent learns to make decisions by receiving Rewards or penalties for
54:23
its actions in an environment so supervised learning is a learning method where the model is
54:28
trained on labeled data unsupervised learning is a type of machine learning
54:34
where it finds hidden patterns in data without using labeled
54:41
inputs you are building a machine learning model on AWS and want to share it securely with the third party partner
54:48
which AWS service would you use to establish a private connection between your VPC and the partner's VPC ensuring
54:54
that the data remains within your a account and is not exposed to the public internet our options are direct connect
55:02
private link Transit Gateway or
55:08
VPN option b AWS privately is the right answer here it enables secure private
55:14
connectivity between vpcs or services within AWS without exposing data to
55:20
public internet AWS Direct Connect provides a dedicated physical connection
55:26
between between your on premises Network and AWS Transit Gateway connects multiple
55:31
vpcs and on premises networks through a single Gateway simplifying large scale Network
55:37
management AWS VPN establishes a secure encrypted connection between your on
55:44
promises Network and AWS over the public internet you are training a machine
55:50
learning model on sensitive customer data using AWS sagemaker under the AWS
55:55
shared responsib model which of the following is primarily your responsibility option A securing the aw
56:01
sagemaker infrastructure B protecting underlying operating system of the
56:06
sagemaker instance C ensuring security for customer data stored in S3 D
56:13
patching the AWS sagemaker software option C ensuring security for
56:21
customer data stored in S3 is the right answer here under the AWS shared
56:27
responsibility model you as an AWS user are primarily responsible for the security of your customer data including
56:34
implementing appropriate security measures and access controls for data stored on
56:40
S3 when implementing the generative AI security scoping Matrix which of the following factors should be assessed to
56:47
determine the level of risk associated with the generative AI project option A
56:52
the model's computational efficiency B the sensitivity of the data used to train the model C inference latency or D
57:01
the number of parameters in the model option b is the right answer here
57:08
the sensitivity of the data used to train the model is critical factor for security other options like
57:14
computational efficiency inference latency and parameters are important but do not directly impact security