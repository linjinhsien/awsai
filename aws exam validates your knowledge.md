 This exam validates your knowledge of artificial intelligence, machine learning, generative AI concepts and use cases. An organization is developing a model to predict the price of a product based on various features like size, weight, brand and manufacturing date. Which machine learning approach would be best suited for this task. Our options are classification, regression, clustering and dimensionality reduction. Option B, regression is the right answer here. Clustering groups similar data points together. Dimensionality reduction reduces the number of features, regression predicts a continuous numerical value, and classification predicts categorical outcomes. A company is expanding its use of artificial intelligence. Which core principles should they prioritize to establish clear guidelines, oversight and accountability for AI development and use? Our options are bias prevention, accuracy and reliability, data protection and security or governance. Option D, governance is the right answer here. It encompasses the overall framework for managing AI systems including policies, procedures and decision making process. Other options are important too but for oversight, governance is the best answer here. A company is starting to use generative AI on AWS to ensure responsible AI practices which tool can provide them with guidance and information. Option A, AWS Marketplace, B, AWS AI Service Cards, C, SageMaker and D, Bedrock. Option B, AWS AI Service Cards is the right answer here. AI Service Cards offer detailed information on specific AWS AI services including their intended use cases, limitations and responsible design considerations. AWS Marketplace is a platform for buying and selling AI services and tools. AWS SageMaker is a machine learning implementation service. AWS Bedrock provides access to a variety of foundation models. What is the primary purpose of feature engineering in machine learning? Option A. To ensure consistent performance of the model B. To evaluate the model's performance C. To gather and pre-process data features D. To transform data and create variables or features for the model Option D. To transform data and create variables or features for the model is the right answer here. A small company wants to use machine learning to predict customer churn but they lack an expert dedicated data science team. Which AWS tool can help them build models easily without extensive coding? Option A Amazon SageMaker Jumpstart, B SageMaker Studio, C SageMaker Canvas or D SageMaker Data Wrangler. C SageMaker Canvas or D SageMaker Data Wrangler Option C SageMaker Canvas is the right answer here. It is a visual interface for building machine learning models without writing code. SageMaker Jumpstart provides pre-built models and templates. SageMaker Studio is a web-based IDE for machine learning development SageMaker Canvas is a visual interface for building machine learning models without writing code and SageMaker Data Wrangler prepares and cleans data for machine learning a financial institution is developing a fraud detection model the project lead announced that they would be using MLOps how do you explain ML Ops in the context of this project? Option A, a tool for visualizing ML model performance. B, a set of practices for managing the entire lifecycle of ML systems. C, a process for deploying and maintaining ML models in production D A framework for building and training ML models Option B A set of practices for managing the entire lifecycle of ML systems is the right answer here. ML Ops encompasses the full lifecycle including development, deployment, monitoring and maintenance of machine learning models which AWS service can be used to create a knowledge based chatbot that can answer questions about companies products and services using the company's internal documents as a source of information option a Amazon SageMaker B Amazon Q business. Amazon Polly or D. Amazon Recognition Option B. Amazon QBusiness is the right answer here. It allows businesses to create intelligent chatbots and search interfaces that can answer questions based on their enterprise documents and data. SageMaker is a managed platform for building, training and deploying machine learning models. Amazon Polly is a text-to-speech service that converts text into natural sounding speech. Recognition is a deep learning based image and video analysis service. A development team needs to select a service for storing and querying vector embeddings. Which AWS service is best suited for this? Option A Glue Data Catalog, B Amazon S3, C Redshift, or D OpenSearch Service C Redshift or D Open Search Service Option D Amazon Open Search Service is the right answer here. Here are some services that support vector based storage and search. Amazon Open Search, Amazon Memory DB, Amazon Aurora RDS Postgres, Amazon Kendra, and there are other products like Pinecone, Milvus, VV8, and Qdrant. An organization wants to evaluate the security and compliance practices of AWS services used by vendors selling AI products. Which AWS service can help them access AWS compliance reports and certifications. Our options are AWS organization, Amazon inspector, AWS CloudRail or AWS artifact. Option B AWS artifact is the right answer here. It allows users to access a centralized repository of AWS compliance reports, attestations and certifications. This includes reports related to security, privacy and regulatory compliance. The other options, AWS organization, Amazon inspector or AWS cloud trail are not relevant here. A machine learning model performs well on training data but poorly on new data. What is the likely problem? Option A Overfitting B Underfitting C Insufficient training data or D Poor data quality Option A Overfitting is the right answer here. So overfitting is the most likely cause of high accuracy on training data but low accuracy on testing data. This occurs when a model becomes overly complex and learns the training data too well to the point where it can't generalize to new or unseen data. All the other options B, C and D will result in poor accuracy on both the training and new data. A company wants to improve the quality of large language model responses by accessing external information. Which method requires the least amount of development effort? Option A few-shot learning, B zero-shot learning, C retrieval augmented generation or RAG or D fine-tuning. Option C retrieval augmented generation is the right answer here. It can retrieve and incorporate external information into prompts that are sent to LLM. information into prompts that are sent to LLL. A model has been trained to recognize handwritten digits in images. However, the model is not accurate. A ML expert has advised that EPOC value should be increased. What is EPOC in the context of machine learning? Option A, a measure of accuracy of a model during training. Option B, a single pass through the entire training data set by the model option c the process of splitting the data set into training and testing sets option d number of layers in a neural network option b is the right answer here, a single pass through the entire training data set by the model. During each epoch, the model processes all the data once, adjusting its parameters based on the loss or error calculated after each pass. Multiple epochs are typically used to iteratively improve the model's performance. Which of the following is considered a hyper parameter in a machine learning model? Option A. Weights of the neural network. B. Learning rate of the optimization algorithm. C. Output of the activation function or D. Predictions made by the model. Option B. Learning rate of the optimization algorithm is the right or D. Predictions made by the model Option B. Learning rate of the optimization algorithm is the right answer here In machine learning, the learning rate is a hyperparameter that determines the size of the steps the model takes when adjusting its weights during training Specifically, it controls how much to change the model's parameters in response to the error or loss between the predicted output and the actual target. So choosing an appropriate learning rate is crucial for the effective training and convergence of the model. Other options A, C and D do not refer to hyperparameters. A model tends to give very similar outputs even when you vary the input slightly. Which inference time parameter can be adjusted to make it a little more creative? Option A, learning rate. B, batch size. C, temperature. Or D, epochs. Option C, temperature is the right answer here. It controls the randomness of predictions by adjusting the probability distribution used during sampling. Lower temperature makes the model more deterministic while higher temperature increases randomness and diversity. Other options A, B and D that is learning rate, batch size and epochs. These are parameters that are used at training time and not inference time. So here are training time hyperparameters. Learning rate controls the step size taken during optimization. Batch size determines the number of samples processed at once during training. Epochs refers to the number of times the entire data set is passed through the model during training. It can improve models accuracy. Regularization these are techniques used to prevent over fitting and optimizer is algorithm used to update model weights. Here are inference time parameters. Temperature controls the randomness of generated text. Top K sampling selects only the top K most likely tokens for output. Top P sampling selects tokens for output based on their cumulative probability. Beam search explores multiple candidate sequences. Greedy search selects the most likely token for output at each step. You are evaluating a language generation model on various tasks related to text generation. To assess the quality of the generated text, which evaluation metric best measures its semantic similarity to human written text? Our options are A. BERT score B. BLUE C. Perplexity or D. Rouge Option A. BERT score is the right answer here. This metric leverages a pre-trained language model to calculate semantic similarity between text. It is particularly well suited for tasks like machine translation and summarization where semantic meaning is crucial. A developer is designing an AI system and needs a solution that provides comprehensive tools for analyzing and explaining model predictions. Which AWS service is specifically designed to enhance transparency and explainability in this context? Option A, SageMaker Clarify, B, SageMaker Debugger, C, SageMaker Autopilot, or D SageMaker Data Wrangler Option A SageMaker Clarify is the right answer here. SageMaker Clarify provides tools for analyzing model predictions and detecting bias, enhancing transparency and explainability. SageMaker Debugger provides tools for debugging and profiling machine learning models. SageMaker Autopilot automatically trains and tunes machine learning models. SageMaker Data Wrangler simplifies data preparation and transformation for machine learning. A company plans to train and build its own foundation model what are potential drawbacks of this approach against using a pre-trained foundation model option a more complex implementation process b reduced performance c risk of higher hallucination d increased development cost Option A and D are the right answers here, that is more complex implementation process and increased development cost. A company wants to generate content using an existing popular pre-trained AI model. They have limited AI expertise and don't want to manage the model themselves. Which AWS service would best suit their needs? Option A, Amazon Textract, B, Amazon Comprehend, C, Amazon Bedrock or D, Amazon SageMaker. Option C, Amazon Bedrock is the right answer here. Bedrock is a managed service that provides access to pre-trained foundation models, making it ideal for companies that want to leverage AI without the need for extensive technical expertise or infrastructure. Amazon Textract extracts text, handwriting, and data from scanned documents. Amazon Comprehend analyzes text for sentiment, entities, key phrases, and more. Amazon Bedrock provides access to pre-trained foundation models for tasks like text generation. Amazon SageMaker is a fully managed platform for building, training, and deploying machine learning models. What type of training data would be most suitable to fine-tune a model to respond to questions in a certain format and style? Option A. Columnar data set. B. Label data. C. Transcription logs. D. Text page of prompts and responses. Option D, text base of prompts and responses is the right answer here. This will allow the model to learn to respond to prompts in a preferred form and style. A company needs to log API calls to Amazon Bedrock for compliance, including details about the API call, the user and the timestamp. Which AWS service can assist with this option a cloud trail be cloud watch see I am and D security hub option a cloud trail is the right answer here it can record API calls made to AWS services including the details of the API call the user who made it and the timestamp making it suitable for tracking API activity for compliance purposes. Amazon CloudWatch monitors and collects metrics and logs. IAM and Security Hub are security related services. A data science team wants to improve a model's performance. They want to increase the amount and diversity of data used for training and modify the algorithm's learning rate. Which combination of ML pipeline steps will meet these requirements? Option A, data augmentation, B, model monitoring, C, feature engineering, or D, hyperparameter tuning. Engineering or D. Hyperparameter Tuning Option A and D. Data Augmentation and Hyperparameter Tuning is the right answer here Data augmentation is a technique used to artificially increase the size and diversity of a data set by applying various transformations to existing data points thereby providing the model with more examples to learn from. Hyper parameter tuning involves adjusting the learning rate which is a crucial parameter that controls how quickly the model updates its weights during training. Model monitoring does not help here. Feature engineering involves selecting or creating new features from data for training and is not relevant here. A company wants to ensure that the content generated by their Amazon Bedrock powered application adheres to their ethical guidelines and avoids harmful or offensive content. Which AWS service can help them implement these safeguards option A Amazon SageMaker B Amazon Comprehend C Textract or D guardrails for Amazon Bedrock option D guardrails for Amazon Bedrock is the right answer here guardrails ensure responsible use of foundation models aligning with organizational compliance requirements it prevents harmful biased or inappropriate content generation by filtering and monitoring outputs SageMaker is a fully managed platform for building training and deploying machine learning models comprehend analyzes text for sentiment, entities, key phrases and more. Textract extracts text handwriting and data from scanned documents. Your company is training a machine learning model on a data set stored in S3 that contains sensitive customer information. How can you ensure that any sensitive information in the data is removed or anonymized before training the model? Option A, use S3 encryption to protect the data at rest. Option B, use Amazon Macie to identify sensitive information within the dataset. Option C, use S3 access controls to limit access to authorized personnel, option B implement data masking techniques to replace sensitive information. Option B and D are the right answers here, that is use Amazon Macie to identify sensitive information within the data set and implement data masking techniques to replace sensitive information within the dataset and implement data masking techniques to replace sensitive information. Other options help secure the data but are not relevant to removing or anonymizing sensitive data. A company wants to use Generative AI to create marketing slogans for their products. Why should the company carefully review all generated slogans? Option A, Generative AI may generate slogans that are too long and difficult to remember. Option B, Generative AI may struggle to capture the unique brand identity of the company. Option C, Generative AI may produce slogans that are inappropriate or misleading. Option D, Generative AI may require extensive training data to generate effective slogans option C generating AI may produce slogans that are inappropriate or misleading is the right answer here Gen AI can sometimes generate content that is biased or offensive this may be due to the biases present in the data they are trained on. Therefore, human oversight is essential to ensure the quality and appropriateness of the generated slogans. Your company is training machine learning models on EC2 instances. You are concerned about the security of these models and want to identify potential vulnerabilities in the underlying infrastructure. Which AWS service can help you scan your EC2 instances for vulnerabilities? Options AWS X-Ray, Amazon CloudWatch, Amazon Inspector or AWS Config. Option C, Amazon Inspector is the right answer here. It is a security service that helps you identify vulnerabilities in your EC2 instances. AWS X-Ray allows you to trace requests as they flow through your application components. Amazon CloudWatch is a monitoring service that provides metrics, logs and alarms for your AWS resources AWS Config is a service that tracks configuration changes to your AWS resources A machine learning model for loan approvals performs better for applicants from urban areas because the training data contains more approval examples from urban areas. What type of bias is this an example of? Option A Sampling bias B Algorithm bias C Observer bias or D Recency bias Option A Sampling bias is the right answer here. Sampling bias occurs when the training data is not representative of all scenarios. Algorithm bias happens when the algorithm itself is biased. Observer bias, the person collecting or labeling data introduces bias. Recency bias, the model gives more weight to recent data. For a data set of social network connections where each user has relationships with multiple other users which machine learning algorithm is most suitable for classifying these interconnected relationships into predefined categories? Option A linear regression, B decision trees, Cural Networks or D Logistic Regression Option C Graph Neural Networks is the right answer here. Linear Regression predicts a continuous target variable based on linear relationships between features. Decision trees classifies data by recursively splitting it into subsets based on feature values forming a tree-like model of decisions. Graph neural networks classifies interconnected data by leveraging graph structures to capture relationships between nodes. Logistic regression predicts the probability of binary outcome by modeling the relationship between features and the outcome using a logistic function. A robot is tasked with navigating a maze to reach a goal. Which machine learning paradigm would be most suitable for training the robot to learn the optimal path via self-learning, trial and error. Our options are supervised learning, unsupervised learning, random learning or reinforcement learning. Option D reinforcement learning is the right answer here. Supervised learning trains a model to predict outputs based on labeled input data. Unsupervised learning trains a model to find patterns or relationships within unlabeled data. Reinforcement learning trains a model to perform well on a new domain-specific task with limited labeled data. Which of the following approaches would be most efficient and suitable? Option A – Continued pre-training with additional unlabeled data Option B – Fine-tuning with label data from the new domain. Option C, using a pre-trained model without any further adjustment. Option D, training from scratch with new label data. Option B is the right answer here. Fine-tuning is appropriate for adapting a model to perform well on a specific task with label data. It caters to domain specific training most efficiently. It could also be referred as domain adaption fine tuning. If you are a small startup with unpredictable workloads and need to experiment with different foundation models, which pricing model would be most suitable for you on amazon bedroom option a on demand b provision throughput c model customization d custom contracts option a on demand is the right answer here. On demand, pay as you go for flexible usage, ideal for unpredictable workloads. Provision throughput, reserved capacity for predictable workloads ensuring consistent performance. Model customization, trading models on your data for specific tasks, enhancing performance and knowledge. In the context of natural language processing which of the following is a fundamental unit of text used to represent words or sub words? Option A token, B vector embedding, C n-gram, D vocabulary. Option A token is the right answer here. Token is the right answer here. Token is the basic unit of text used to represent words or subwords after tokenization. Vector embedding is a numerical representation of tokens or words in a continuous vector space. N-gram is a sequence of n consecutive words or characters from a text used to analyze and understand patterns in language. Vocabulary is a collection of all unique words that a model recognizes and processes. A developer is creating an AI system to predict customer churn. To ensure transparency, they need to document key details about the model. Which AWS tool is best suited for this task? Option A, Amazon SageMaker Clarify. B, AWS AI Service Cards. C, SageMaker Model Cards. And D, SageMaker Jumpstart. Correct answer here is option C, Amazon SageMaker Model Cards. Correct answer here is option C, Amazon SageMaker Model Cards. AWS AI Service Cards provides information about different AWS AI services. Amazon SageMaker Clarify helps identify and mitigate biases in machine learning models. Amazon SageMaker Jumpstart provides pre-built models and templates for common machine learning tasks and Amazon SceneMaker model cards documents and shares information about machine learning models such as their intended use, training data and evaluation metrics. An engineer is training a machine learning model in order to prevent underfitting or overfitting how should the model be trained with data? Option A with high bias and high variance B low bias and low variance C high bias and low variance D low bias and high variance Option B with low bias and low variance is the right answer here. The goal is to strike a balance between bias and variance when training model. Low bias ensures that the model can accurately learn the underlying patterns in the data preventing underfitting. Low variance ensures that the model generalizes well to new unseen data preventing overfitting. Model overfitting is a case of low bias and high variance. Here the model learned training data too well. Therefore, it is unable to generalize on test data and the model has become complex. Model underfitting is a case of high bias and low variance. Here the model did not learn enough from training data. Its performance is poor both on training and test data. It is a simplistic model. Bias refers to models failure to capture patterns in training data and variance refers to model sensitivity to fluctuations and noise in data. You are customizing a large language model for a specific domain. Which approach is most effective for tailoring the model's knowledge and accuracy to this domain? Option A, fine tuning. B, few-shot learning. C, retrieval augmented generation or RAG. D, zero-shot learning. or RAG, B. Zero-shot learning Option A. Fine-tuning is the right answer here. Here we train the pre-trained model on domain specific data to improve its performance and knowledge with respect to that domain. Fine-tuning trains the pre-trained model on domain specific data to improve its performance and knowledge. Retrieval Augmented Generation enhances the capabilities of a large language model by incorporating information from external sources. Few-shot learning trains the model on a small amount of data. Zero-shot learning makes predictions on tasks or domains that the model hasn't been explicitly trained on. Which of the following is an example of hallucination in large language models? A. Overfitting B. Underfitting C. Generating false or misleading information D. bias. Option C. generating false or misleading information is the right answer here. Large language models can sometimes generate responses that are factually incorrect, inconsistent or simply nonsensical. This is known as hallucination. Which is a foundation model developed by Amazon available via Bedrock? Amazon Titan, Lex, Poly or Connect? Option A Amazon Titan is the right answer here. Amazon Titan is a powerful foundation model from Amazon. Amazon Lex, this is a service for building conversational interfaces such as chatbots and voice assistants. Amazon Polly is a service for converting text to speech. And Amazon Connect is a cloud-based contact center service. Which of the following algorithms are commonly used for classification tasks in machine learning? Our options are support vector machine, XGBoost, K-means or MeanShift. Option A, SVM and B, XGBoost are the right answers here. and B, XGBoost are the right answers here. Both SVM and XGBoost are supervised learning algorithms primarily used for classification, while K-Means and MeanShift are unsupervised learning algorithms primarily used for clustering. Given a large dataset intended for inference where latency is not a factor, which SageMaker model inference type would you choose for cost-effective predictions? Our options are Realtime, Batch, On-Demand Serverless or Asynchronous Option B, Batch is the right answer here. It is good for processing large data sets offline in a cost effective manner. Rear time is good for immediate predictions with low latency. On-demand serverless is ideal for workloads which have idle periods between traffic spots and can tolerate cold starts. No need to manage infrastructure. The asynchronous method queues incoming requests and processes them asynchronously. It is good where processing times are long and queuing requests make sense. What is the primary purpose of Amazon queue developer? Our options are option A to manage AWS infrastructure, B to assess developers with coding tasks and queries, C to optimize optimize database performance, D, to automate software testing. Option B, to assess developers with coding tasks and queries is the right answer here. Amazon Q Developer is an AI powered coding companion designed to assess developers with various coding tasks and queries. It can help with code generation, debugging, and answering questions about AWS services and best practices. What kind of prompt attack is this? Explain why a false statement is true considering that it's usually known to be false. Option A. Jailbreaking, B. Prompt poisoning, C. Adversarial prompting, or D. Fine-tuning Option C. Adversarial prompting is the right answer here. Jailbreaking is about crafting inputs to bypass a model's safety constraints leading to prohibited or harmful responses. leading to prohibited or harmful responses. Prompt poisoning is about introducing biased or misleading data into prompts to influence or degrade the model's output. Adversarial prompting is about creating inputs designed to exploit model's weakness and produce incorrect or unintended responses. You are building a text summarization tool which metric is best for measuring how well it captures the key points of the original text. Our options are a. BERT score b. ROUGE c. Word error rate d. Bilingual evaluation understudy or BLUE Bilingual Evaluation Understudy or BLUE Option B. Rouge is the right answer here. It is specifically designed for evaluating text summaries. BLUE metric is used to evaluate the quality of text which has been machine translated from one natural language to another. BERT score measures semantic similarity between text. Rouge is specifically designed for evaluating text summaries. Word error rate is primarily used in speech recognition to measure the accuracy of transcriptions. Perplexity measures the complexity of a language model. An AI customer service agent is unable to accurately identify customer intent based on customer message. You can improve its performance by using training data in which format? Option A customer message and customer intent. B customer message and agent response. C customer intent and agent response D. Agent response and customer intent Option A. Customer message and customer intent is the right answer here because this format directly provides the model with the information it needs to learn the relationship between customer messages and their corresponding intents. How are users typically charged for using a foundation model? Option A – Number of input tokens B – Number of output tokens C – Model architecture or D – Inference latency Option A and B is the right answer here users are generally charged based on the number of tokens processed during input and output in the models inference process model architecture and inference latency are not typically used to charge model users which AWS AI service can be used to extract health data from unstructured text such as clinical notes and medical records? Option A, Amazon Comprehend Medical, B, Amazon Transcribe Medical, C, Amazon Health Lake, or D, Amazon Recognition. A. Amazon Comprehend Medical is the right answer here. Amazon Comprehend Medical extracts medical information from text. Amazon Health Lake stores and analyzes health data. Amazon Transcribe Medical converts medical speech to text. Which type of machine learning model is specifically designed to generate new data that resembles existing data? Option A autoencoder, B generative adversarial network, C decision tree or D support vector machine. Option B generative adversarial network is the right answer here. It is specifically designed for generative tasks such as creating new images, music or text. Autoencoders are used for dimensionality reduction and data reconstruction. Decision trees and SVMs are primarily used for classification and regression tasks. Users are going to use log prompts to ask questions from their large language model. What key aspect should be considered while selecting the LLM to use? Option A Inference latency, B Maximum context window, C Model size or D Training data Model size or D. Training data Option B. Maximum context window is the right answer here. A large maximum context window allows the LLM to process and understand more information from the prompt, leading to more comprehensive and relevant response. Which of the following best describes the primary purpose of Amazon SageMaker feature store? Option A, to automatically train and deploy machine learning models. B, to store and manage features for machine learning workflows. C, to provide a marketplace for pre-trained machine learning models. D, to optimize the performance of SageMaker training jobs. Option B, to store and manage features for machine learning workflows is the right answer here. Amazon SageMaker Feature Store is a fully managed, purpose-built repository to store, update, retrieve, and share machine learning features. This ensures enabling feature sharing among data scientists and machine learning engineers and ensuring feature consistency across training and inference. A healthcare organization is developing an AI-powered diagnostic tool to assist in early detection of a rare disease. With respect to regulatory compliance concerns, which of the following is least relevant? Option A, ensuring the AI system is unbiased and does not discriminate against certain patient demographics. B, minimizing operational expenses of the AI system. C, ensuring the AI system is transparent in its decision-making process. D. Preventing the AI system from being used for unauthorized purposes. Option B. Minimizing operational expenses of the AI system is the right answer here. Regulatory compliance in healthcare especially for diagnostic tools focuses on factors like unbiased decision making, transparency and security while minimizing operational expenses is important for overall efficiency, it is less relevant to regulatory compliance compared to other options. You are a large enterprise with massive amount of unstructured data scattered across various internal systems. You want to provide your employees with a powerful search tool that can understand natural language queries and return accurate relevant results. Which AWS service would best meet this need? Option A. Amazon Redshift, B. Amazon Lex, C. Amazon Kendra or D. Amazon DynamoDB. Option C C Amazon Kendra is the right answer here. It is a managed service for searching unstructured data using natural language queries. Amazon Redshift is a data warehouse service. Amazon Lex is a service for building conversational interfaces with natural language understanding. Amazon DynamoDB is a NoSQL database service for storing and retrieving data. A data scientist is working on a project that requires a rapid prototyping and experimentation with various machine learning algorithms. Which AWS service would be most suitable for this task? Option A Amazon SageMaker Ground Truth, B Amazon Elastic Compute Cloud or EC2 C Amazon SageMaker Autopilot or D Amazon Bedrock Option C Amazon SageMaker Autopilot is the right answer here. It is a fully managed machine learning service that automates the process of building, training and deploying models. Amazon SageMaker Ground Truth is a service for creating high quality training data sets for machine learning models. EC2 provides scalable compute capacity in the cloud and Amazon Bedrock is a fully managed service that makes foundational models accessible to developers. A large company wants to create an application for their sales managers that can reason, perform multi-step tasks, and provide insightful responses from their enterprise data. Which AWS service would be most suitable for this task? Our options are Amazon Lex, SageMaker, Bedrock KnowledgeBases, or Bedrock Agents. Option B, Amazon Bedrock Agrock agents is the right answer here. They can leverage both LLMs and knowledge bases to provide answers based on enterprise data. Amazon Lex is specifically designed for building conversational interfaces. SageMaker is a platform for building and training machine learning models while knowledge bases can serve as a data source for other services. A company wants to analyze customer reviews to identify common themes and sentiments. Which AWS service can the company use to meet this requirement. Our options are Amazon Connect, Comprehend, Translate or Transcribe. Option B, Amazon Comprehend is the right answer here. It can extract insights from text including entities, sentiments and key phrases. Amazon Connect is used for building contact centers. Amazon Translate translates text between languages, while Amazon Transcribe can transcribe audio into text. A company wants to transform data from one format to another to prepare it for machine learning tasks, which AWS service is best suited for this data transformation. AWS Glue, Amazon Translate, AWS Config or Amazon Kinesis. Option A, AWS Glue is the right answer here. AWS Glue is a fully managed ETL service that can be used to transform data from one format to another. Amazon Translate is used for translating text between languages. AWS Config is used for tracking configuration changes in AWS resources. And Amazon Kinesis is a service for processing real-time data streams. A company wants to deploy a trained machine learning model for real-time inference. Which AWS service would be most suitable for this purpose? Option A Amazon SageMaker Jumpstart, B Personalize, C EC2 or D SageMaker Endpoints. Option D SageMaker Endpoints is the right answer here. It is specifically designed for real-time inference allowing you to deploy your trained machine learning model and make predictions in real-time. SageMaker Jumpstart provides pre-trained models and algorithms. Amazon Personalize is a service for building personalized recommendations. Amazon EC2 is a general purpose compute service. A company has deployed a machine learning model for customer sentiment analysis to ensure the model's accuracy and reliability which AWS services should be used for monitoring and human review. Option A, Amazon Bedrock, B, SageMaker Model Monitor, C, SageMaker Ground Truths, you option a amazon bedrock b sage maker model monitor c sage maker ground truths or d amazon a2i so option b amazon sage maker model monitor and option d amazon a2i are the right answers here sage maker model monitor is a service for monitoring the performance of machine learning models in production. Amazon A2i or Amazon Augmented AI is a service for incorporating human review into machine learning workflows. Amazon Bedrock provides access to a variety of foundation models. While Amazon Ground Truth is a service for creating high quality training data sets for machine learning models. A machine learning specialist is training a large deep learning model on a massive data set in Amazon SageMaker. A single GPU may not handle this well. Which SageMaker feature can help optimize the training process for large models and data sets option a incremental training B hyper parameter tuning C pipe mode or D model parallelism option D model parallelism is the right answer here it splits a large model across multiple GPUs or instances allowing each GPU to process a portion of the model. Incremental training is useful for updating a model with new data. Hyperparameter tuning optimizes model's performance by trying different combinations of hyperparameters and pipe mode is a feature for streaming data and efficient data management during training. You are working with a large data set with many features. To improve your model's performance and computational efficiency, you need to simplify the data without losing significant information. Which technique would be most effective for achieving this goal? Option A, dimensionality reduction. B, feature engineering. C, data Augmentation D. Data Cleaning Option A. Dimensionality Reduction is the right answer here. This technique reduces the number of features in a dataset while preserving essential information. Feature engineering involves creating new features from existing ones to improve model performance. Data augmentation is a technique that creates additional training data by modifying existing data such as rotating images or adding noise to audio. Data cleaning involves removing or correcting errors, inconsistencies or missing values in the data. You want to generate highly detailed images based on text descriptions. Which AI model specifically designed for generative tasks and capable of producing high quality diverse outputs would be most suitable for this task? Option A. Generative adversarial networks B. Recurrent neural networks C. Convolutional neural networks or D. Stable diffusion Option D. Stable diffusion is the right answer here. Generative adversarial networks is a class of machine learning models that use a competitive process to generate new data. Recurrent neural networks is a type of neural network that can process sequential data such as text or time series. Convolutional neural networks are a type of neural networks that are particularly effective for processing images and other grid-like data. While Stable Diffusion is a model that is specifically designed for generative tasks such as image generation. A company has a system that generates vector embeddings from product data. They want to improve the speed and accuracy of finding similar products. Which AWS services are best suited for implementing vector search to optimize the system? Option A, open search service, B, Redshift, C, Neptune or D, DocumentDB. Option A, C and D are the right answers here. Open search, Neptune and DocumentDB support handling vector data and performing similarity searches via vector search. Redshift is a data warehousing service. A bank receives numerous loan applications daily. The loan processing team manually extracts information from these applications which is time consuming. The goal is to automate this process using AI tools. Which AWS service would be useful here? Option A Amazon Recognition B Textract C Translate or D Transcribe Option B Amazon Textract is the right answer here. Amazon Textract can extract text from scanned documents. Amazon Recognition analyzes images and videos to identify objects, scenes and faces. Amazon Translate translates text between languages and Amazon Transcribe can transcribe speech to text. A healthcare company wants to develop a machine learning model to predict the likelihood of a patient developing diabetes based on various health indicators. Which of the following metrics would be most appropriate for evaluating the model's performance? Our options are accuracy, precision, F1 score, recall sensitivity, or area under ROC curve. Precision, F1 score, Recall sensitivity or area under ROC curve? Options D and E are the right answers here. Here we are dealing with a classification problem. In this scenario, both recall sensitivity and AUC would be appropriate metrics for evaluating the model's performance. Accuracy measures the overall correct predictions. Precision measures the proportion of positive predictions that are actually positive. F1 score is a harmonic mean of precision and recall providing a balanced measure of performance. Recall sensitivity measures the proportion of actual positive instances that were correctly predicted. Area under ROC curve or AUC measures the models ability to distinguish between positive and negative instances. An organization has trained a deep learning model on a large data set of general images. They now want to apply the same model to classify medical images with a smaller data set. Which machine learning technique would be most suitable in this scenario? Our options are reinforcement learning, transfer learning, supervised learning or unsupervised learning. Option B, transfer learning is the right answer here. It is a technique that reuses a pre-trained model for a new but related task, often with some fine tuning on new data. Reinforcement learning is a type of machine learning where an agent learns to make decisions by receiving rewards or penalties for its actions in an environment. Supervised learning is a learning method where the model is trained on labeled data. Unsupervised learning is a type of machine learning where it finds hidden patterns in data without using labeled inputs. You are building a machine learning model on AWS and want to share it securely with a third party partner. Which AWS service would you use to establish a private connection between your VPC and the partner's VPC, ensuring that the data remains within your AWS account and is not exposed to the public internet? Our options are Direct Connect, Private Link, Transit Gateway or VPN. Option B, AWS Private Link is the right answer here. It enables secure private connectivity between VPCs or services within AWS without exposing data to public internet. AWS Direct Connect provides a dedicated physical connection between your on-premises network and AWS. Transit Gateway connects multiple VPCs and on-premises networks through a single gateway simplifying large-scale network management. AWS VPN establishes a secure encrypted connection between your on-premises network and AWS over the public internet. You are training a machine learning model on sensitive customer data using AWS SageMaker. Under the AWS shared responsibility model, each of the following is primarily your responsibility. Option A, securing the AWS SageMaker infrastructure. B, protecting underlying operating system of the SageMaker instance. C, ensuring security for customer data stored in S3. D, patching the AWS SageMaker software. Option C, ensuring security for customer data stored in S3 is the right answer here. C. Ensuring security for customer data stored in S3 is the right answer here. Under the AWS Shared Responsibility Model, you as an AWS user are primarily responsible for the security of your customer data, including implementing appropriate security measures and access controls for data stored on S3. When implementing the Generative AI Security Scoping matrix, which of the following factors should be assessed to determine the level of risk associated with a Generative AI project? Option A – The model's computational efficiency B – The sensitivity of the data used to train the model C – Inference latency or D – The number of parameters in the model option B is the right answer here the sensitivity of the data used to train the model is critical factor for security other options like computational efficiency inference latency and parameters are important but do not directly impact security Impact Security.